%!TEX root = main.tex

\section{Herleitung eines A-posteriori-Fehlerschätzers} % (fold)
\label{sec:herleitung}

\subsection{Benötigte Grundlagen aus der Funktionalanalysis} % (fold)
\label{sub:ben_tigte_grundlagen_aus_der_funktionalanalysis}

Für die Herleitung des Fehlerschätzers ist eine Aussage aus der Funktionalanalysis von großer Bedeutung, weswegen wir mit ihr beginnen.

\begin{Definition}[Hilbertraum]
    \label{def:hilbertraum}
    Sei ein reeller Vektorraum $H$ mit einem Skalarprodukt $\skprod{\cdot}{\cdot}$ gegeben.
    Ist $H$ bezüglich der durch das Skalarprodukt induzierten Norm $\norm{\cdot}_H = \sqrt{\skprod{\cdot}{\cdot}}$ vollständig, dann heißt $H$ Hilbertraum.
\end{Definition}

\begin{Satz}[Rieszscher Darstellungssatz]
    \label{satz:rieszscher_darstellungssatz}
    Sei $(H, \skprod{\cdot}{\cdot})$ ein Hilbertraum. Dann existiert zu jedem stetigen linearen Funktional $f \colon H \to \bbR$ genau ein $y \in H$, sodass gilt
    \begin{equation}
        f(x) = \skprod{x}{y} \quad \fa x \in H,
    \end{equation}
    und
    \begin{equation}
         \norm{f}_{H'} := \sup_{x \in H \setminus \left\{ 0 \right\}} \frac{\abs{f(x)}}{\norm{x}_H} = \norm{y}_H.
    \end{equation}
\end{Satz}

\subsection{Wiederholung} % (fold)
\label{sub:wiederholung}

Es sei eine Teilmenge $\Omega \subset \mathbb{R}^d$ und ein Hilbertraum $X^e$ von Funktionalen auf $\Omega$ gegeben.
Oftmals betrachten wir $d \in \left\{ 1, 2, 3 \right\}$ und $H^1_0(\Omega) \subset X^e \subset H^1(\Omega)$.
Der sogenannte Parameterraum $\mathcal D \subset \mathbb{R}^n$ sei eine kompakte Menge.

Wir betrachten das folgende parametrische Variationsproblem:
\begin{addmargin}[2em]{2em}
    Sei $\mu \in \mathcal D$ gegeben. Gesucht ist eine Lösung $u^e(\mu) \in X^e$, sodass
    \begin{equation}
        a(u^e(\mu), v; \mu) = f(v; \mu) \quad \fa v \in X^e
    \end{equation}
    gilt.
    Bestimme $s^e(\mu) := l(u^e(\mu); \mu)$.
\end{addmargin}
Dabei sei $a \colon X^e \times X^e \times \mathcal D \to \mathbb{R}$ eine parametrische stetige Bilinearform und $f, l \colon X^e \times \mathcal D \to \mathbb{R}$ seien parametrische stetige lineare Funktionale.
Wir beschränken uns im Weiteren auf den Fall $f = l$.

Als weitere Einschränkung fordern wir, dass $a$ und $f$ parametrisch affin sind.
Dies bedeutet, dass natürliche Zahlen $Q_a$ und $Q_f$ existieren, sowie Abbildungen
\begin{align}
    \Theta_a^k(\mu) &\colon \mathcal D \to \mathbb{R}, \quad 1 \leq k \leq Q_a, \\
    \Theta_f^j(\mu) &\colon \mathcal D \to \mathbb{R}, \quad 1 \leq j \leq Q_f,
\end{align}
stetige Bilinearformen
\begin{equation}
    a^k \colon X^e \times X^e \to \mathbb{R}, \quad 1 \leq k \leq Q_a,
\end{equation}
und stetige lineare Funktionale
\begin{equation}
    f^j \colon X^e \to \mathbb{R}, \quad 1 \leq j \leq Q_f,
\end{equation}
sodass
\begin{align*}
    a(w, v; \mu) = \sum_{k=1}^{Q_a} \Theta_a^k(\mu) a^k(w, v)
    \qquad \text{und} \qquad
    f(v; \mu)    = \sum_{j=1}^{Q_f} \Theta_f^j(\mu) f^j(v)
\end{align*}
für alle $w, v \in X^e$ und alle $\mu \in \mathcal D$ gilt.

Wir haben bereits gesehen, dass das obige parametrische Variationsproblem unter bestimmten Voraussetzungen wohldefiniert ist, zum Beispiel, wenn $a$ koerziv ist oder die inf-sup-Bedingung erfüllt.
Auch hier nehmen wir eine Einschränkung vor und betrachten hauptsächlich den Fall, dass $a$ koerziv ist.

Die Schreibweise $\bullet^e$ verwenden wir, um zu kennzeichnen, dass es sich dabei um die \emph{exakte} Variante des Variationsproblems handelt.

Handelt es sich, wie oben angenommen, bei $a$ um eine koerzive Bilinearform, dann ist $a$ insbesondere auch positiv definit.
Ist $a$ zudem symmetrisch, dann definiert $a(\cdot, \cdot; \mu)$ für jedes $\mu \in \mathcal D$ ein Skalarprodukt auf $X^e$.
Ist $a$ nicht symmetrisch, dann können wir zum symmetrischen Anteil
\begin{equation}
    a_s(w, v; \mu) = \frac{1}{2} \Big[ a(w, v; \mu) + a(v, w; \mu) \Big], \quad w, v \in X^e,
\end{equation}
übergehen, da dieser ebenfalls koerziv, und damit positiv definit ist.
Wir erhalten somit eine Familie von Skalarprodukten auf $X^e$ durch
\begin{equation}
    \skprod{w}{v}_{\mu} = a(w, v; \mu), \quad v, w \in X^e, \quad \mu \in \mathcal D,
\end{equation}
die jeweils eine Norm
\begin{equation}
    \norm{w}_{\mu} = \sqrt{\skprod{w}{w}_{\mu}} = \sqrt{a(w, w; \mu)}, \quad w \in X^e, \quad \mu \in \mathcal D,
\end{equation}
induzieren.
Wir wählen aus dem Parameterraum $\mathcal D$ einen Referenzparameter $\bar \mu \in \mathcal D$ aus und bezeichnen das zugehörige Skalarprodukt als $X^e$-Skalarprodukt und analog die induzierte Norm als $X^e$-Norm
\begin{equation}
    \skprod{w}{v}_{X^e} = a(w, v; \bar \mu),
    \qquad
    \norm{w}_{X^e} = \sqrt{\skprod{w}{w}_{X^e}}.
\end{equation}
Die Wahl dieses $\bar \mu \in \mathcal D$ hat zwar keine Auswirkungen auf die Ergebnisse der Reduzierte-Basis-Methode, wohl aber auf die Präzision der hier behandelten A-posteriori-Fehlerschätzer.

Mit $X \subset X^e$, wobei $\dim X = \mathcal N \in \mathbb{N}$, bezeichnen wir den endlichdimensionalen Galerkin-Ansatzraum und die dazugehörige Lösung mit $u(\mu) \in X$.
Das Vorwort \emph{Galerkin-} wird hier gewählt, da in der Praxis hierfür oft die Finite-Elemente-Methode verwendet wird, dies aber nicht immer der Fall sein muss.
Wir schreiben $s(\mu) = f(u(\mu); \mu)$ für das Ausgabefunktional zu der Galerkin-Lösung für $\mu$.

Als endlichdimensionaler Unterraum des Hilbertraums $X^e$ ist $X$ abgeschlossen und damit selbst ein Hilbertraum.
Insbesondere vererben sich die obigen Skalarprodukte und Normen alle von $X^e$ auf $X$.
Da auf endlichdimensionalen Vektorräumen alle Normen äquivalent sind, erhalten wir damit insbesondere, dass $X$ bezüglich jedem der obigen Skalarprodukte ein Hilbertraum ist.

Analog bezeichnen wir mit $X_N \subset X$, $\dim X_N = N \ll \mathcal N$, den Reduzierte-Basis-Ansatzraum und mit $u_N(\mu) \in X_N$ die Reduzierte-Basis-Lösung des obigen Variationsproblems.
Auch $X_N$ ist ein Hilbertraum.
Das Ausgabefunktional für die Reduzierte-Basis-Lösung schreiben wir als $s_N(\mu) = f(u_N(\mu); \mu)$.

\subsection{Herleitung} % (fold)
\label{sub:herleitung}

Für diesen Abschnitt sei $a$ eine koerzive, parametrisch affine, symmetrische und stetige Bilinearform und $f$ ein parametrisch affines, stetiges, lineares Funktional.

Wir beschränken uns nun auf die Herleitung eines Fehlerschätzers für den Fehler zwischen der Lösung $u(\mu)$ der Galerkin-Methode und der Lösung $u_N(\mu)$ der Reduzierte-Basis-Methode. Wegen der Dreiecksungleichung gilt
\begin{equation}
    \norm{u^e(\mu) - u_N(\mu)}_X \leq \norm{u^e(\mu) - u(\mu)}_X + \norm{u(\mu) - u_N(\mu)}_X,
\end{equation}
außerdem kann der Fehler $\norm{u^e(\mu) - u(\mu)}_X$ durch die Wahl eines geeigneten Galerkin-Ansatzraumes $X$ beeinflusst werden und wird deswegen von nun an nicht mehr betrachtet.

Wir definieren die Differenz zwischen der Lösung $u(\mu)$ und der Reduzierte-Basis-Lösung $u_N(\mu)$ als
\begin{equation}
    \label{eq:fehler_fe_und_rb_lsg}
    e \colon \mathcal D \to X, ~
    e(\mu) := u(\mu) - u_N(\mu).
\end{equation}
Einsetzen in die Bilinearform $a$ und Ausnutzen der Bilinearität von $a$ liefert
\begin{align}
    a(e(\mu), v; \mu)
    &= a(u(\mu) - u_N(\mu), v; \mu)
    = a(u(\mu), v; \mu) - a(u_N(\mu), v; \mu) \\
    &= f(v; \mu) - a(u_N(\mu), v; \mu)
\end{align}
für alle $v \in X$.
Wir bezeichnen nun
\begin{equation}
    \label{eq:def_residuum}
    r(v; \mu) = f(v; \mu) - a(u_N(\mu), v; \mu), \quad v \in X,
\end{equation}
als Residuum.
Das Residuum hängt offensichtlich nur noch von der Reduzierte-Basis-Lösung $u_N(\mu) \in X_N$ ab, nicht aber von der Lösung $u(\mu) \in X$ der Galerkin-Methode.

\begin{Lemma}
    Das Residuum $r(\cdot; \mu) \colon X \to \mathbb{R}$ ist für alle $\mu \in \mathcal D$ ein stetiges lineares Funktional. Es gilt also $r(\cdot; \mu) \in X'$ für alle $\mu \in \mathcal D$.

    \begin{Beweis}
    Sei $\mu \in \mathcal D$ beliebig.
    Die Linearität von $r(\cdot; \mu)$ ergibt sich direkt aus der Linearität von $f(\cdot; \mu)$ und der Bilinearität von $a(\cdot, \cdot; \mu)$.
    Ebenso überträgt sich auch die Stetigkeit von $f(\cdot; \mu)$ und $a(\cdot, \cdot; \mu)$ auf $r(\cdot; \mu)$.
    \end{Beweis}
\end{Lemma}

Da $X$ mit dem Skalarprodukt $\skprod{\cdot}{\cdot}_X$ ein Hilbertraum ist und $r(\cdot; \mu) \in X'$ gilt, erhalten wir durch den Rieszschen Darstellungssatz \ref{satz:rieszscher_darstellungssatz} ein $\hat e(\mu) \in X$, sodass
\begin{equation}
    \label{eq:residuum_riesz}
    r(v; \mu) = \skprod{\hat e(\mu)}{v}_X \quad \fa v \in X
\end{equation}
gilt.
Insbesondere können wir also
\begin{equation}
    \label{eq:a_von_e_gl_skprod_e}
    a(e(\mu), v; \mu) = \skprod{\hat e(\mu)}{v}_X \quad \fa v \in X
\end{equation}
schreiben und es gilt $\hat e(\bar \mu) = e(\bar \mu)$.
Außerdem liefert der Rieszsche Darstellungssatz \ref{satz:rieszscher_darstellungssatz}
\begin{equation}
    \label{eq:residuum_norm_gl_hat_e_norm}
    \norm{r(\cdot; \mu)}_{X'} := \sup_{v \in X \setminus \left\{ 0 \right\}} \frac{\abs{r(v; \mu)}}{\norm{v}_X} = \norm{\hat e(\mu)}_X.
\end{equation}
Diese Darstellung der Norm des Residuums durch das Element $\hat e(\mu)$ ist essentiell für die spätere Berechnung des A-posteriori-Fehlerschätzers.

Als weiteren Bestandteil benötigen wir für unseren Fehlerschätzer die Koerzivität von $a$.
Sei also
\begin{equation}
            \alpha(\mu) = \inf_{w \in X} \frac{a(w, w; \mu)}{\norm{w}^2_X} > 0, \quad \mu \in \mathcal D,
\end{equation}
die Koerzivitätskonstante von $a(\cdot, \cdot; \mu)$ über $X$.
Mit $\alpha_{\text{LB}}(\mu) > 0$ bezeichnen wir eine positive untere Schranke für dieses $\alpha(\mu)$, es gelte also
\begin{equation}
    \label{eq:alpha_lb}
    0 < \alpha_{\text{LB}}(\mu) \leq \alpha(\mu) \quad \fa \mu \in \mathcal D.
\end{equation}
In Kapitel \ref{sec:berechnung_des_fehlersch_tzers} werden wir behandeln, wie man eine solche Schranke bestimmt.
Zunächst aber leiten wir unter Verwendung dieser Schranke den gesuchten A-posteriori-Fehlerschätzer her.

\begin{Definition}
    Unter den oben genannten Voraussetzungen können wir folgende Fehlerschätzer definieren.
    Für den Fehler $e(\mu)$ in der $\mu$-Norm (oder auch Energienorm) beziehungsweise in der $X$-Norm setzen wir
    \begin{equation}
        \Delta^{\text{en}}_N(\mu) := \frac{\norm{\hat e(\mu)}_X}{\sqrt{\alpha_{\text{LB}(\mu)}}}
        , \quad
        \Delta_N(\mu) := \frac{\norm{\hat e(\mu)}_X}{\alpha_{\text{LB}(\mu)}}
        , \quad \mu \in \mathcal D.
    \end{equation}
    Für den Fehler $s(\mu) - s_N(\mu)$ des Ausgabefunktionals definieren wir
    \begin{equation}
        \Delta^s_N(\mu) := \frac{\norm{\hat e(\mu)}_X^2}{\alpha_{\text{LB}(\mu)}}, \quad \mu \in \mathcal D.
    \end{equation}
\end{Definition}

% subsection herleitung (end)

\subsection{Eigenschaften der A-posteriori-Fehlerschätzer} % (fold)
\label{sub:eigenschaften_der_a_}

Wir beweisen zunächst einige Aussagen über die gewonnenen Fehlerschätzer, bevor wir uns um die Berechnung dieser kümmern.

Dazu benötigen wir, wenn auch nur für die für uns weniger interessante obere Schranke der Fehlerschätzer, die Stetigkeitskonstante
\begin{equation}
    \gamma(\mu) = \sup_{w \in X} \sup_{v \in X} \frac{a(w, v; \mu)}{\norm{w}_X \norm{v}_X} < \infty, \quad \mu \in \mathcal D,
\end{equation}
von $a(\cdot, \cdot; \mu)$ über $X$.

\begin{Satz}
    Die Fehlerschätzer sind nach unten und nach oben durch den tatsächlichen Fehler beschränkt, denn es gilt
    \begin{equation}
        \norm{e(\mu)}_{\mu} \leq \Delta^{\text{en}}_N(\mu) \leq \sqrt{\frac{\gamma(\mu)}{\alpha_{\text{LB}}(\mu)}} \norm{e(\mu)}_{\mu}, \qquad
        \norm{e(\mu)}_X \leq \Delta_N(\mu) \leq \frac{\gamma(\mu)}{\alpha_{\text{LB}}(\mu)}\norm{e(\mu)}_X
    \end{equation}
    und
    \begin{equation}
         s(\mu) - s_N(\mu) \leq \Delta^s_N(\mu) \leq \frac{\gamma(\mu)}{\alpha_{\text{LB}}(\mu)} \left( s(\mu) - s_N(\mu) \right)
    \end{equation}
    für alle $\mu \in \mathcal D$.

    \begin{Beweis}
        Sei $\mu \in \mathcal D$ beliebig.
        Wir betrachten für den Fehler $e(\mu)$ als Erstes den $\mu$-Norm-Fehlerschätzer.
        Nach Definition, Gleichung \eqref{eq:a_von_e_gl_skprod_e} und der Cauchy-Schwarz-Ungleichung gilt
        \begin{equation}
            \label{eq:beweis_satz_untere_schranke_a}
            \norm{e(\mu)}_{\mu}^2 = a(e(\mu), e(\mu); \mu) = \skprod{\hat e(\mu)}{e(\mu)}_X \leq \norm{\hat e(\mu)}_X \norm{e(\mu)}_X.
        \end{equation}
        Aus Ungleichung \eqref{eq:alpha_lb} und der Koerzivität von $a$ erhalten wir
        \begin{equation}
            \label{eq:beweis_satz_untere_schranke_b}
            \alpha_{\text{LB}}(\mu) \norm{e(\mu)}^2_X \leq \alpha(\mu) \norm{e(\mu)}^2_X \leq a(e(\mu), e(\mu); \mu) = \norm{e(\mu)}^2_{\mu}
        \end{equation}
        und daraus
        \begin{equation}
            \label{eq:beweis_satz_untere_schranke_c}
            \sqrt{\alpha_{\text{LB}}(\mu)} \norm{e(\mu)}_X \leq \norm{e(\mu)}_{\mu}.
        \end{equation}
        Die beiden Ungleichungen \eqref{eq:beweis_satz_untere_schranke_a} und \eqref{eq:beweis_satz_untere_schranke_c} zusammen liefern
        \begin{equation}
            \norm{\hat e(\mu)}_X \norm{e(\mu)}_X \geq \norm{e(\mu)}_{\mu} \norm{e(\mu)}_{\mu} \geq \sqrt{\alpha_{\text{LB}}(\mu)} \norm{e(\mu)}_X \norm{e(\mu)}_{\mu}.
        \end{equation}
        Kürzen von $\norm{e(\mu)}_X$ und Anwenden dieser Ungleichung auf $\Delta^\text{en}_N$ liefert die Abschätzung nach unten, denn
        \begin{equation}
            \Delta^{\text{en}}_N(\mu)
            =
            \frac{\norm{\hat e(\mu)}_X}{\sqrt{\alpha_{\text{LB}(\mu)}}}
            \geq
            \frac{\sqrt{\alpha_{\text{LB}(\mu)}}\norm{e(\mu)}_{\mu}}{\sqrt{\alpha_{\text{LB}(\mu)}}}
            =
            \norm{e(\mu)}_{\mu}.
        \end{equation}
        Wegen der Stetigkeit von $a$ gilt
        \begin{equation}
            \label{eq:beweis_satz_obere_schranke_a}
            \norm{\hat e(\mu)}_{\mu} = \sqrt{a(\hat e(\mu), \hat e(\mu); \mu)} \leq \sqrt{\gamma(\mu)}\norm{\hat e(\mu)}_X.
        \end{equation}
        Außerdem erhalten wir mit Hilfe von Gleichung \eqref{eq:a_von_e_gl_skprod_e} und der Cauchy-Schwarz-Ungleichung
        \begin{equation}
            \label{eq:beweis_satz_obere_schranke_notag}
            \norm{\hat e(\mu)}^2_X = \skprod{\hat e(\mu)}{\hat e(\mu)}_X = a(e(\mu), \hat e(\mu); \mu)
            = \skprod{e(\mu)}{\hat e(\mu)}_{\mu} \leq \norm{e(\mu)}_{\mu} \norm{\hat e(\mu)}_{\mu}.
        \end{equation}
        Zusammen mit \eqref{eq:beweis_satz_obere_schranke_a} liefert dies
        \begin{equation}
            \label{eq:beweis_satz_obere_schranke_b}
            \norm{\hat e(\mu)}^2_X \leq \norm{e(\mu)}_{\mu} \norm{\hat e(\mu)}_{\mu} \leq \sqrt{\gamma(\mu)} \norm{e(\mu)}_{\mu} \norm{\hat e(\mu)}_X
        \end{equation}
        und damit nach Kürzen von $\norm{\hat e(\mu)}_X$ auch die Abschätzung nach oben für den $\mu$-Norm-Fehlerschätzer
        \begin{equation}
            \Delta_N^{\text{en}}(\mu) = \frac{\norm{\hat e(\mu)}_X}{\sqrt{\alpha_{\text{LB}}(\mu)}} \leq \sqrt{\frac{\gamma(\mu)}{\alpha_{\text{LB}}(\mu)}} \norm{e(\mu)}_{\mu}.
        \end{equation}

        Für den Fehlerschätzer $\Delta_N(\mu)$ der $X$-Norm verwenden wir die obigen Ungleichungen \eqref{eq:beweis_satz_untere_schranke_a} und \eqref{eq:beweis_satz_untere_schranke_b} und erhalten direkt
        \begin{equation}
            \Delta_N(\mu) = \frac{\norm{\hat e(\mu)}_X}{\alpha_\text{LB}(\mu)} \geq \norm{e(\mu)}_X.
        \end{equation}
        Die Abschätzung nach oben ergibt sich aus \eqref{eq:beweis_satz_obere_schranke_b} und \eqref{eq:beweis_satz_obere_schranke_a}, mit $e(\mu)$ statt $\hat e(\mu)$,
        \begin{equation}
            \Delta_N(\mu)
            = \frac{\norm{\hat e(\mu)}_X}{\alpha_\text{LB}(\mu)}
            \leq \frac{\sqrt{\gamma(\mu)}\norm{e(\mu)}_{\mu} }{\alpha_\text{LB}(\mu)}
            \leq \frac{\gamma(\mu)}{\alpha_\text{LB}(\mu)} \norm{e(\mu)}_X.
        \end{equation}

        Es fehlt nun nur noch der Fehlerschätzer $\Delta^s_N$. Betrachte dazu für $v \in X_N$ beliebig
        \begin{equation}
            a(e(\mu), v; \mu) = a(u(\mu), v; \mu) - a(u_N(\mu), v; \mu) = f(v; \mu) - f(v; \mu) = 0.
        \end{equation}
        Da $a$ symmetrisch ist und $u_N(\mu) \in X_N$, gilt damit
        \begin{equation}
            a(u_N(\mu), e(\mu); \mu) = a(e(\mu), u_N(\mu); \mu) = 0.
        \end{equation}
        Außerdem erinnern wir an
        \begin{equation}
            s(\mu) = f(u(\mu); \mu), \qquad s_N(\mu) = f(u_N(\mu); \mu).
        \end{equation}
        Zusammen ergibt dies
        \begin{align}
            s(\mu) - s_N(\mu) &= f(u(\mu); \mu) - f(u_N(\mu); \mu) = f(e(\mu); \mu) \\
            &= a(u(\mu), e(\mu); \mu) = a(u(\mu), e(\mu); \mu) - a(u_N(\mu), e(\mu); \mu)\\
            &= a(e(\mu), e(\mu); \mu) = \norm{e(\mu)}_{\mu}^2.
        \end{align}
        Da außerdem nach Definition
        \begin{equation}
            \Delta^s_N(\mu) = \left( \Delta^{\text{en}}_N(\mu) \right)^2,
        \end{equation}
        folgen die Abschätzungen von $\Delta^s_N$ aus dem Beweis für $\Delta^\text{en}_N$.
    \end{Beweis}
\end{Satz}

Als nächstes führen wir den Begriff der Effektivität der Fehlerschätzer ein und übertragen den obigen Satz darauf.

\begin{Korollar}
    \label{korollar:effektivitaeten}
    Wir definieren für unsere Fehlerschätzer jeweils die Effektivität durch
    \begin{equation}
        \eta^{\text{en}}_N(\mu) := \frac{\Delta^{\text{en}}_N(\mu)}{\norm{e(\mu)}_{\mu}}
        , \qquad
        \eta_N(\mu) = \frac{\Delta_N(\mu)}{\norm{e(\mu)}}_X
        , \qquad
        \eta^{s}_N(\mu) = \frac{\Delta^s_N(\mu)}{s(\mu) - s_N(\mu)}.
    \end{equation}
    Dann gilt
    \begin{align}
        1 \leq \eta^{\text{en}}_N(\mu) \leq \sqrt{\frac{\gamma(\mu)}{\alpha_{\text{LB}}(\mu)}}, \qquad
        1 \leq \eta_N(\mu),~\eta^{s}_N(\mu) \leq \frac{\gamma(\mu)}{\alpha_{\text{LB}}(\mu)},
    \end{align}
    für alle $\mu \in \mathcal D$.
\end{Korollar}
Man sieht leicht ein, dass es für unsere Anwendungszwecke wünschenswert ist, wenn die Effektivitäten nahe der Eins liegen, unsere Fehlerschätzer die tatsächlichen Fehler also nur minimal überschätzen.

% section herleitung (end)
